{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python382jvsc74a57bd031f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6",
   "display_name": "Python 3.8.2 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading cora dataset...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from utils import load_data\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_data() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import GCN\n",
    "model = GCN(features.size(1),16,labels.max().item()+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n\nTest Loss: 1.929010, Test Acc: 0.192000\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from utils import accuracy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "opt = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "for i in range(200):\n",
    "    opt.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    break\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])    \n",
    "    loss_train.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "\n",
    "    if True:\n",
    "        model.eval()\n",
    "        output = model(features, adj)\n",
    "        print (\"Epoch: {:02d}, Train Loss: {:04f}, Train Acc: {:04f}, Val Loss: {:04f}, Val Acc: {:04f}\".format(i + 1,loss_train, acc_train,loss_val, acc_val))\n",
    "\n",
    "model.eval()\n",
    "loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "\n",
    "print (\"\\n\\nTest Loss: {:04f}, Test Acc: {:04f}\".format(loss_test, acc_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[-2.1474, -1.7823, -1.7856,  ..., -1.9158, -1.9822, -2.0573],\n",
       "         [-2.1906, -1.7625, -1.7984,  ..., -1.8105, -2.1123, -1.9147],\n",
       "         [-2.1855, -1.7826, -1.7718,  ..., -1.8943, -2.1218, -1.9156],\n",
       "         ...,\n",
       "         [-2.1981, -1.7547, -1.7803,  ..., -1.8671, -2.0756, -1.9538],\n",
       "         [-2.1389, -1.8049, -1.8379,  ..., -1.8130, -2.0077, -1.9813],\n",
       "         [-2.1825, -1.7641, -1.7655,  ..., -1.9090, -2.0601, -1.9854]],\n",
       "        grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor(indices=tensor([[   0,    8,   14,  ..., 1389, 2344, 2707],\n",
       "                        [   0,    0,    0,  ..., 2707, 2707, 2707]]),\n",
       "        values=tensor([0.1667, 0.1667, 0.0500,  ..., 0.2000, 0.5000, 0.2500]),\n",
       "        size=(2708, 2708), nnz=13264, layout=torch.sparse_coo),\n",
       " tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
       "          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
       "          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
       "          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
       "          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
       "          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
       "          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
       "         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
       "         126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139]),\n",
       " tensor([ 500,  501,  502,  503,  504,  505,  506,  507,  508,  509,  510,  511,\n",
       "          512,  513,  514,  515,  516,  517,  518,  519,  520,  521,  522,  523,\n",
       "          524,  525,  526,  527,  528,  529,  530,  531,  532,  533,  534,  535,\n",
       "          536,  537,  538,  539,  540,  541,  542,  543,  544,  545,  546,  547,\n",
       "          548,  549,  550,  551,  552,  553,  554,  555,  556,  557,  558,  559,\n",
       "          560,  561,  562,  563,  564,  565,  566,  567,  568,  569,  570,  571,\n",
       "          572,  573,  574,  575,  576,  577,  578,  579,  580,  581,  582,  583,\n",
       "          584,  585,  586,  587,  588,  589,  590,  591,  592,  593,  594,  595,\n",
       "          596,  597,  598,  599,  600,  601,  602,  603,  604,  605,  606,  607,\n",
       "          608,  609,  610,  611,  612,  613,  614,  615,  616,  617,  618,  619,\n",
       "          620,  621,  622,  623,  624,  625,  626,  627,  628,  629,  630,  631,\n",
       "          632,  633,  634,  635,  636,  637,  638,  639,  640,  641,  642,  643,\n",
       "          644,  645,  646,  647,  648,  649,  650,  651,  652,  653,  654,  655,\n",
       "          656,  657,  658,  659,  660,  661,  662,  663,  664,  665,  666,  667,\n",
       "          668,  669,  670,  671,  672,  673,  674,  675,  676,  677,  678,  679,\n",
       "          680,  681,  682,  683,  684,  685,  686,  687,  688,  689,  690,  691,\n",
       "          692,  693,  694,  695,  696,  697,  698,  699,  700,  701,  702,  703,\n",
       "          704,  705,  706,  707,  708,  709,  710,  711,  712,  713,  714,  715,\n",
       "          716,  717,  718,  719,  720,  721,  722,  723,  724,  725,  726,  727,\n",
       "          728,  729,  730,  731,  732,  733,  734,  735,  736,  737,  738,  739,\n",
       "          740,  741,  742,  743,  744,  745,  746,  747,  748,  749,  750,  751,\n",
       "          752,  753,  754,  755,  756,  757,  758,  759,  760,  761,  762,  763,\n",
       "          764,  765,  766,  767,  768,  769,  770,  771,  772,  773,  774,  775,\n",
       "          776,  777,  778,  779,  780,  781,  782,  783,  784,  785,  786,  787,\n",
       "          788,  789,  790,  791,  792,  793,  794,  795,  796,  797,  798,  799,\n",
       "          800,  801,  802,  803,  804,  805,  806,  807,  808,  809,  810,  811,\n",
       "          812,  813,  814,  815,  816,  817,  818,  819,  820,  821,  822,  823,\n",
       "          824,  825,  826,  827,  828,  829,  830,  831,  832,  833,  834,  835,\n",
       "          836,  837,  838,  839,  840,  841,  842,  843,  844,  845,  846,  847,\n",
       "          848,  849,  850,  851,  852,  853,  854,  855,  856,  857,  858,  859,\n",
       "          860,  861,  862,  863,  864,  865,  866,  867,  868,  869,  870,  871,\n",
       "          872,  873,  874,  875,  876,  877,  878,  879,  880,  881,  882,  883,\n",
       "          884,  885,  886,  887,  888,  889,  890,  891,  892,  893,  894,  895,\n",
       "          896,  897,  898,  899,  900,  901,  902,  903,  904,  905,  906,  907,\n",
       "          908,  909,  910,  911,  912,  913,  914,  915,  916,  917,  918,  919,\n",
       "          920,  921,  922,  923,  924,  925,  926,  927,  928,  929,  930,  931,\n",
       "          932,  933,  934,  935,  936,  937,  938,  939,  940,  941,  942,  943,\n",
       "          944,  945,  946,  947,  948,  949,  950,  951,  952,  953,  954,  955,\n",
       "          956,  957,  958,  959,  960,  961,  962,  963,  964,  965,  966,  967,\n",
       "          968,  969,  970,  971,  972,  973,  974,  975,  976,  977,  978,  979,\n",
       "          980,  981,  982,  983,  984,  985,  986,  987,  988,  989,  990,  991,\n",
       "          992,  993,  994,  995,  996,  997,  998,  999, 1000, 1001, 1002, 1003,\n",
       "         1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015,\n",
       "         1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027,\n",
       "         1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039,\n",
       "         1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051,\n",
       "         1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063,\n",
       "         1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075,\n",
       "         1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087,\n",
       "         1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099,\n",
       "         1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111,\n",
       "         1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123,\n",
       "         1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135,\n",
       "         1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147,\n",
       "         1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159,\n",
       "         1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171,\n",
       "         1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183,\n",
       "         1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195,\n",
       "         1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207,\n",
       "         1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219,\n",
       "         1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231,\n",
       "         1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243,\n",
       "         1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255,\n",
       "         1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267,\n",
       "         1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279,\n",
       "         1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291,\n",
       "         1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303,\n",
       "         1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315,\n",
       "         1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327,\n",
       "         1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339,\n",
       "         1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351,\n",
       "         1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363,\n",
       "         1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375,\n",
       "         1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387,\n",
       "         1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399,\n",
       "         1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411,\n",
       "         1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423,\n",
       "         1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435,\n",
       "         1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447,\n",
       "         1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459,\n",
       "         1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471,\n",
       "         1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483,\n",
       "         1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495,\n",
       "         1496, 1497, 1498, 1499]))"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "output,features, adj, idx_train, idx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}